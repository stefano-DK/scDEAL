{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import preprocessing\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (average_precision_score,\n",
    "                             classification_report, mean_squared_error, r2_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import  nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sampling as sam\n",
    "#import utils as ut\n",
    "import trainers as t\n",
    "from models import (AEBase,PretrainedPredictor, PretrainedVAEPredictor, VAEBase)\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stefano.cardinale/scDEAL/scDEAL'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "dim_au_out = 256\n",
    "select_drug = 'I.BET.762'\n",
    "na = 1\n",
    "data_path='data/GDSC2_expression.csv'\n",
    "label_path='data/GDSC2_label_192drugs_binary.csv'\n",
    "test_size = 0.2\n",
    "valid_size = 0.2\n",
    "g_disperson = None\n",
    "model_path = 'saved/models/bulk_predictor_'\n",
    "bulk_encoder = 'saved/models/bulk_encoder_ae_256_test.pkl' #This are the weights for the pretrained AE model\n",
    "log_path = 'saved/logs/log'\n",
    "batch_size = 200\n",
    "encoder_hdims = \"256,256\".split(\",\")\n",
    "preditor_hdims = \"128,64\".split(\",\")\n",
    "reduce_model = \"AE\"\n",
    "sampling = None\n",
    "PCA_dim = 0\n",
    "pretrain = 1\n",
    "freeze_pretrain = 0\n",
    "VAErepram = 1\n",
    "\n",
    "encoder_hdims = list(map(int, encoder_hdims) )\n",
    "preditor_hdims = list(map(int, preditor_hdims) )\n",
    "load_model = bool(0)\n",
    "\n",
    "preditor_path = model_path + reduce_model  + select_drug + '.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging and std out\n",
    "now=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "#ut.save_arguments(now)\n",
    "\n",
    "out_path = log_path+now+\".err\"\n",
    "log_path = log_path+now+\".log\"\n",
    "\n",
    "out=open('./debug.log',\"w\")\n",
    "#sys.stderr=out\n",
    "logging.basicConfig(filename='./debug.log', encoding='utf-8', level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "data_r=pd.read_csv(data_path,index_col=0)\n",
    "label_r=pd.read_csv(label_path,index_col=0)\n",
    "label_r=label_r.fillna(na)\n",
    "\n",
    "\n",
    "now=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "selected_idx = label_r.loc[:,select_drug]!=na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(g_disperson!=None):\n",
    "    hvg,adata = ut.highly_variable_genes(data_r,min_disp=g_disperson)\n",
    "    # Rename columns if duplication exist\n",
    "    data_r.columns = adata.var_names\n",
    "    # Extract hvgs\n",
    "    data = data_r.loc[selected_idx.index,hvg]\n",
    "else:\n",
    "    data = data_r.loc[selected_idx.index,:]\n",
    "\n",
    "# Do PCA if PCA_dim!=0\n",
    "if PCA_dim !=0 :\n",
    "    data = PCA(n_components = PCA_dim).fit_transform(data)\n",
    "else:\n",
    "    data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefano.cardinale/opt/anaconda3/envs/scDEAL/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels\n",
    "label = label_r.loc[selected_idx.index,select_drug]\n",
    "data_r = data_r.loc[selected_idx.index,:]\n",
    "\n",
    "# Scaling data\n",
    "mmscaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "data = mmscaler.fit_transform(data)\n",
    "label = label.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(label)\n",
    "dim_model_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(np.std(data))\n",
    "logging.info(np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split traning valid test set\n",
    "X_train_all, X_test, Y_train_all, Y_test = train_test_split(data, label, test_size=test_size, random_state=42)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_all, Y_train_all, test_size=valid_size, random_state=42)\n",
    "# sampling method\n",
    "if sampling == None:\n",
    "    X_train,Y_train=sam.nosampling(X_train,Y_train)\n",
    "    logging.info(\"nosampling\")\n",
    "elif sampling ==\"upsampling\":\n",
    "    X_train,Y_train=sam.upsampling(X_train,Y_train)\n",
    "    logging.info(\"upsampling\")\n",
    "elif sampling ==\"downsampling\":\n",
    "    X_train,Y_train=sam.downsampling(X_train,Y_train)\n",
    "    logging.info(\"downsampling\")\n",
    "elif  sampling==\"SMOTE\":\n",
    "    X_train,Y_train=sam.SMOTEsampling(X_train,Y_train)\n",
    "    logging.info(\"SMOTE\")\n",
    "else:\n",
    "    logging.info(\"not a legal sampling method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[515, 512]\n",
      "[512, 515]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "h_dims=[512]\n",
    "hidden_dims = deepcopy(h_dims)\n",
    "hidden_dims.insert(0,515)\n",
    "print(hidden_dims)\n",
    "\n",
    "hidden_dims.reverse()\n",
    "print(hidden_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z5/6hy18q9n2m5346tc4cw_k80r0000gn/T/ipykernel_12251/745907193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         encoder,loss_report_en = t.train_AE_model(net=encoder,data_loaders=dataloaders_pretrain,\n\u001b[1;32m     50\u001b[0m                                     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function_e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                     n_epochs=epochs,scheduler=exp_lr_scheduler_e,save_path=bulk_encoder)\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce_model\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"VAE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         encoder,loss_report_en = t.train_VAE_model(net=encoder,data_loaders=dataloaders_pretrain,\n",
      "\u001b[0;32m~/scDEAL/scDEAL/trainers.py\u001b[0m in \u001b[0;36mtrain_AE_model\u001b[0;34m(net, data_loaders, optimizer, loss_function, n_epochs, scheduler, load, save_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/scDEAL/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/scDEAL/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select the Training device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "logging.info(device)\n",
    "#torch.cuda.set_device(device)\n",
    "\n",
    "# Construct datasets and data loaders\n",
    "X_trainTensor = torch.FloatTensor(X_train).to(device)\n",
    "X_validTensor = torch.FloatTensor(X_valid).to(device)\n",
    "X_testTensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "Y_trainTensor = torch.LongTensor(Y_train).to(device)\n",
    "Y_validTensor = torch.LongTensor(Y_valid).to(device)\n",
    "\n",
    "# Preprocess data to tensor\n",
    "train_dataset = TensorDataset(X_trainTensor, X_trainTensor)\n",
    "valid_dataset = TensorDataset(X_validTensor, X_validTensor)\n",
    "\n",
    "X_trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "X_validDataLoader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# construct TensorDataset\n",
    "trainreducedDataset = TensorDataset(X_trainTensor, Y_trainTensor)\n",
    "validreducedDataset = TensorDataset(X_validTensor, Y_validTensor)\n",
    "\n",
    "trainDataLoader_p = DataLoader(dataset=trainreducedDataset, batch_size=batch_size, shuffle=True)\n",
    "validDataLoader_p = DataLoader(dataset=validreducedDataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders_train = {'train':trainDataLoader_p,'val':validDataLoader_p}\n",
    "\n",
    "if(bool(pretrain)!=False):\n",
    "    dataloaders_pretrain = {'train':X_trainDataLoader,'val':X_validDataLoader}\n",
    "    if reduce_model == \"VAE\":\n",
    "        encoder = VAEBase(input_dim=data.shape[1],latent_dim=dim_au_out,h_dims=encoder_hdims)\n",
    "    else:\n",
    "        encoder = AEBase(input_dim=data.shape[1],latent_dim=dim_au_out,h_dims=encoder_hdims)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        encoder.cuda()\n",
    "\n",
    "    logging.info(encoder)\n",
    "    encoder.to(device)\n",
    "\n",
    "    optimizer_e = optim.Adam(encoder.parameters(), lr=1e-2)\n",
    "    loss_function_e = nn.MSELoss()\n",
    "    exp_lr_scheduler_e = lr_scheduler.ReduceLROnPlateau(optimizer_e)\n",
    "\n",
    "    if reduce_model == \"AE\":\n",
    "        encoder,loss_report_en = t.train_AE_model(net=encoder,data_loaders=dataloaders_pretrain,\n",
    "                                    optimizer=optimizer_e,loss_function=loss_function_e,\n",
    "                                    n_epochs=epochs,scheduler=exp_lr_scheduler_e,save_path=bulk_encoder)\n",
    "    elif reduce_model == \"VAE\":\n",
    "        encoder,loss_report_en = t.train_VAE_model(net=encoder,data_loaders=dataloaders_pretrain,\n",
    "                        optimizer=optimizer_e,\n",
    "                        n_epochs=epochs,scheduler=exp_lr_scheduler_e,save_path=bulk_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainedPredictor(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=17419, out_features=256, bias=True)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (predictor): Predictor(\n",
       "    (predictor): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (output): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if reduce_model == \"AE\":\n",
    "        model = PretrainedPredictor(input_dim=X_train.shape[1],latent_dim=dim_au_out,h_dims=encoder_hdims, \n",
    "                                hidden_dims_predictor=preditor_hdims,output_dim=dim_model_out,\n",
    "                                pretrained_weights=bulk_encoder,freezed=bool(freeze_pretrain))\n",
    "elif reduce_model == \"VAE\":\n",
    "    model = PretrainedVAEPredictor(input_dim=X_train.shape[1],latent_dim=dim_au_out,h_dims=encoder_hdims, \n",
    "                    hidden_dims_predictor=preditor_hdims,output_dim=dim_model_out,\n",
    "                    pretrained_weights=bulk_encoder,freezed=bool(freeze_pretrain),z_reparam=bool(VAErepram))\n",
    "\n",
    "logging.info(\"Current model is:\")\n",
    "logging.info(model)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# Train prediction model\n",
    "model,report = t.train_predictor_model(model,dataloaders_train,\n",
    "                                        optimizer,loss_function,epochs,exp_lr_scheduler,load=load_model,save_path=preditor_path)\n",
    "dl_result = model(X_testTensor).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_results = np.argmax(dl_result,axis=1)\n",
    "#pb_results = np.max(dl_result,axis=1)\n",
    "pb_results = dl_result[:,1]\n",
    "\n",
    "report_dict = classification_report(Y_test, lb_results, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).T\n",
    "ap_score = average_precision_score(Y_test, pb_results)\n",
    "auroc_score = roc_auc_score(Y_test, pb_results)\n",
    "\n",
    "report_df['auroc_score'] = auroc_score\n",
    "report_df['ap_score'] = ap_score\n",
    "\n",
    "report_df.to_csv(\"saved/logs/\" + reduce_model + select_drug+now + '_report.csv')\n",
    "\n",
    "logging.info(classification_report(Y_test, lb_results))\n",
    "logging.info(average_precision_score(Y_test, pb_results))\n",
    "logging.info(roc_auc_score(Y_test, pb_results))\n",
    "\n",
    "model = DummyClassifier(strategy='stratified')\n",
    "model.fit(X_train, Y_train)\n",
    "yhat = model.predict_proba(X_test)\n",
    "naive_probs = yhat[:, 1]\n",
    "\n",
    "ut.plot_roc_curve(Y_test, naive_probs, pb_results, title=str(roc_auc_score(Y_test, pb_results)),\n",
    "                    path=\"saved/figures/\" + reduce_model + select_drug+now + '_roc.pdf')\n",
    "ut.plot_pr_curve(Y_test,pb_results,  title=average_precision_score(Y_test, pb_results),\n",
    "                path=\"saved/figures/\" + reduce_model + select_drug+now + '_prc.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc0e95d49b90f0a4fb3944c3147f30a7432599c425adf8cde2b7ffa19b0136b1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
